{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 1298\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3bad48f6aa58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_word_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-3bad48f6aa58>\u001b[0m in \u001b[0;36mcreate_word_vectors\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''\n",
    "    text = input_text\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+','',line[0])\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "    text = re.sub(r'RT','',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    cleaned_text = re.sub('(?!^)([A-Z][a-z]+)', r' \\1', text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "\n",
    "# read the input file and create the set of positive examples and negative examples. \n",
    "\n",
    "file=open('cancer_data.tsv')\n",
    "file_out = open(\"cancer_data_processed.tsv\", \"a\")\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2= preprocess_tweet(line[0]).strip().split()\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    line[0]= preprocess_tweet(line[0])\n",
    "    file_out.write(line[0]+'\\t'+line[1])\n",
    "\n",
    "corpus_file = datapath('/home/jaswanth/Coding/DL-2019/assignment_3/cancer_data_processed.tsv')\n",
    "model_gensim = FT_gensim(size=word_embed_size)\n",
    "model_gensim.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "model_gensim.train(\n",
    "    corpus_file=corpus_file, epochs=model_gensim.epochs,\n",
    "    total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words\n",
    ")\n",
    "\n",
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "'''\n",
    "After this you will obtain the following :\n",
    "\n",
    "1) sentences =  List of sentences having the positive and negative examples with all the positive examples first\n",
    "2) y = List of labels with the positive labels first.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running the CNN there are a few things one needs to take care of: (5)\n",
    "\n",
    "1) Pad the sentences so that all of them are of the same length\n",
    "2) Build a vocabulary comprising all unique words that occur in the corpus\n",
    "3) Convert each sentence into a corresponding vector by replacing each word in the sentence with the index in the vocabulary. \n",
    "\n",
    "Example :\n",
    "S1 = a b a c\n",
    "S2 = d c a \n",
    "\n",
    "Step 1:  S1= a b a c, \n",
    "         S2 =d c a </s> \n",
    "         (Both sentences are of equal length). \n",
    "\n",
    "Step 2:  voc={a:1, b:2, c:3, d:4, </s>: 5}\n",
    "\n",
    "Step 3:  S1= [1,2,1,3]\n",
    "         S2= [4,3,1,5]\n",
    "\n",
    "'''\n",
    "sequence_length = max(len(x) for x in sentences)\n",
    "word_vectors = np.zeros((len(sentences),sequence_length,word_embed_size))\n",
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [\"\"] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(sequence_length):\n",
    "            word_vectors[i][j] = model_gensim.wv[sentences[i][j]]\n",
    "    return word_vectors\n",
    "\n",
    "x = create_word_vectors(sentences)\n",
    "\n",
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "sentence_size = x_train.shape[1]\n",
    "\n",
    "epoch = 5\n",
    "batch_size = 20\n",
    "word_embed_size = 200\n",
    "filters = [2,3,4,5]\n",
    "lr = 0.005\n",
    "num_filter = 100\n",
    "\n",
    "input_x = mx.sym.Variable('data')\n",
    "input_y = mx.sym.Variable('softmax_label')\n",
    "\n",
    "conv_input = mx.sym.Reshape(data=input_x, shape=(batch_size, 1, sentence_size, word_embed_size))\n",
    "\n",
    "pooled_outputs = []\n",
    "for filter_size in filters:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size,word_embed_size), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "    \n",
    "total_filters = num_filter * len(filters)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n",
    "num_labels = 2\n",
    "class_weight = mx.sym.Variable('class_weight')\n",
    "class_bias = mx.sym.Variable('class_bias')\n",
    "\n",
    "fully_connected = mx.sym.FullyConnected(data=h_pool, weight=class_weight, bias=class_bias, num_hidden=num_labels)\n",
    "softmax_output = mx.sym.SoftmaxOutput(data=fully_connected, label=input_y, name='softmax')\n",
    "CNN = softmax_output\n",
    "\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "arg_names = CNN.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = CNN.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']:\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = CNN.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: \n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=CNN, data=data, label=label, param_blocks=param_blocks)\n",
    "\n",
    "opt = mx.optimizer.create('rmsprop')\n",
    "opt.lr = lr\n",
    "\n",
    "max_grad_norm = 5.0\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        batch_X = x_train[i:i+batch_size]\n",
    "        batch_Y = y_train[i:i+batch_size]\n",
    "        \n",
    "        if batch_X.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batch_X\n",
    "        cnn_model.label[:] = batch_Y\n",
    "\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        num_correct += sum(batch_Y == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batch_Y)\n",
    "\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "            updater(idx, grad, weight)\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = (num_correct * 100)/ float(num_total)\n",
    "\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "\n",
    "        \n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for i in range(0, x_test.shape[0], batch_size):\n",
    "        batch_X = x_test[i:i+batch_size]\n",
    "        batch_Y = y_test[i:i+batch_size]\n",
    "\n",
    "        if batch_X.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batch_X\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batch_Y == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batch_Y)\n",
    "\n",
    "    test_acc = (num_correct * 100)/float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.2fs, Training Accuracy: %.2f Test Accuracy: %.2f' %(iteration, train_time, train_acc, test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
