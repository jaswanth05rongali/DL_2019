{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NDArray' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-5c148dfcd270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NDArray' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "mx.random.seed(1)\n",
    "ctx = mx.cpu()\n",
    "\n",
    "def preprocess(input_text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", input_text)\n",
    "    cleaned_text = re.sub('(?!^)([A-Z][a-z]+)', r' \\1', text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "file_train = open('train.txt')\n",
    "file_test = open('test.txt')\n",
    "train_data = file_train.readlines()\n",
    "test_data = file_test.readlines()\n",
    "\n",
    "train_sentences = []\n",
    "for line in train_data:\n",
    "    line = preprocess(line).strip().split()\n",
    "    train_sentences.append(line)\n",
    "    \n",
    "test_sentences = []\n",
    "for line in test_data:\n",
    "    line = preprocess(line).strip().split()\n",
    "    test_sentences.append(line)\n",
    "\n",
    "sentences = list(train_sentences)\n",
    "sentences.extend(test_sentences)\n",
    "\n",
    "word_counts = Counter(itertools.chain(*sentences))\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "vocab_size = len(vocabulary)\n",
    "word_vectors =  np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "\n",
    "def one_hots(num_list, vocab_size=vocab_size):\n",
    "    result = nd.zeros((len(num_list),vocab_size), ctx=ctx)\n",
    "    for i,idx in enumerate(num_list):\n",
    "        result[i][idx] = 1.0\n",
    "    return result\n",
    "\n",
    "# def textify(embedding):\n",
    "#     result = \"\"\n",
    "#     indices = nd.argmax(embedding, axis=1).asnumpy()\n",
    "#     for word,index in vocabulary.items():\n",
    "#         for idx in indices:\n",
    "#             if idx == index:\n",
    "#                 result += word\n",
    "#                 result += ' '\n",
    "#     return result\n",
    "\n",
    "for i in range(len(word_vectors)):\n",
    "    if i == 0:\n",
    "        dataset = one_hots(word_vectors[i])\n",
    "    else:\n",
    "        dataset.concat(dataset,one_hots(word_vectors[i]),dim=0)\n",
    "        \n",
    "print(dataset)\n",
    "\n",
    "# train_data = []\n",
    "# train_labels = []\n",
    "# test_data = []\n",
    "# test_labels = []\n",
    "# for i in range(len(word_vectors)):\n",
    "#     if i < 3610:\n",
    "#         train_data.append(dataset[i][:-1])\n",
    "#         train_labels.append(dataset[i][-1])\n",
    "#     else:\n",
    "#         test_data.append(dataset[i][:-1])\n",
    "#         test_labels.append(dataset[i][-1])\n",
    "        \n",
    "# # train_data = np.array(train_data)\n",
    "# # train_labels = np.array(train_labels)\n",
    "# # test_data = np.array(test_data)\n",
    "# # test_labels = np.array(test_labels)\n",
    "    \n",
    "# batch_size = 64\n",
    "# num_batches = len(dataset) // batch_size\n",
    "\n",
    "# train_data.reshape((batch_size,))\n",
    "# print(train_data)\n",
    "\n",
    "\n",
    "\n",
    "# num_inputs = vocab_size\n",
    "# num_hidden = 256\n",
    "# num_outputs = vocab_size\n",
    "\n",
    "# Wxh = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "# Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * .01\n",
    "# bh = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "# Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
    "# by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01\n",
    "\n",
    "# params = [Wxh, Whh, bh, Why, by]\n",
    "\n",
    "# for param in params:\n",
    "#     param.attach_grad()\n",
    "    \n",
    "# def softmax(y_linear):\n",
    "#     lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) \n",
    "#     exp = nd.exp(lin)\n",
    "#     partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
    "#     return exp / partition\n",
    "\n",
    "# def simple_rnn(inputs, state):\n",
    "#     outputs = []\n",
    "#     h = state\n",
    "#     for X in inputs:\n",
    "#         h_linear = nd.dot(X, Wxh) + nd.dot(h, Whh) + bh\n",
    "#         h = nd.tanh(h_linear)\n",
    "#         yhat_linear = nd.dot(h, Why) + by\n",
    "#         yhat = softmax(yhat_linear)\n",
    "#         outputs.append(yhat)\n",
    "#     return (outputs, h)\n",
    "\n",
    "# def cross_entropy(yhat, y):\n",
    "#     return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
    "\n",
    "# def average_ce_loss(outputs, labels):\n",
    "#     assert(len(outputs) == len(labels))\n",
    "#     total_loss = 0.\n",
    "#     for (output, label) in zip(outputs,labels):\n",
    "#         total_loss = total_loss + cross_entropy(output, label)\n",
    "#     return total_loss / len(outputs)\n",
    "\n",
    "# def SGD(params, lr):\n",
    "#     for param in params:\n",
    "#         param[:] = param - lr * param.grad\n",
    "\n",
    "# epochs = 2000\n",
    "# moving_loss = 0.\n",
    "# learning_rate = .5\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     if ((e+1) % 100 == 0):\n",
    "#         learning_rate = learning_rate / 2.0\n",
    "        \n",
    "#     state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "    \n",
    "#     for i in range(num_batches):\n",
    "#         data_one_hot = train_data[i]\n",
    "#         label_one_hot = train_label[i]\n",
    "#         with autograd.record():\n",
    "#             outputs, state = simple_rnn(data_one_hot, state)\n",
    "#             loss = average_ce_loss(outputs, label_one_hot)\n",
    "#             loss.backward()\n",
    "#         SGD(params, learning_rate)\n",
    "\n",
    "#         if (i == 0) and (e == 0):\n",
    "#             moving_loss = np.mean(loss.asnumpy()[0])\n",
    "#         else:\n",
    "#             moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "\n",
    "#     print(\"Epoch %s. Loss: %s\" % (e, moving_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'hi I am'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict_values' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-fd12151b5150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict_values' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "x.keys()[x.values().index(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
